# Disclaimer
This repository contains information collected from various online sources and/or generated by AI assistants. The content provided here is for informational purposes only and is intended to serve as a general reference on various topics.

# PostgreSQL Schema Design and Data Modeling: Best Practices

## Table of Contents

 - [Introduction](#introduction)
  - [1. Tables](#1-tables)
      - [1.1. Naming Conventions](#11-naming-conventions)
      - [1.2. Column Definition](#12-column-definition)
      - [1.3. Primary Keys](#13-primary-keys)
      - [1.4. Foreign Keys](#14-foreign-keys)
      - [1.5. Constraints](#15-constraints)
      - [1.6. Table Partitioning](#16-table-partitioning)
      - [1.7. Example](#17-example)
  - [2. Indexes](#2-indexes)
      - [2.1. Index Types](#21-index-types)
      - [2.2. When to Use Indexes](#22-when-to-use-indexes)
      - [2.3. Composite Indexes](#23-composite-indexes)
      - [2.4. Partial Indexes](#24-partial-indexes)
      - [2.5. Index Maintenance](#25-index-maintenance)
      - [2.6. Example](#26-example)
  - [3. Views](#3-views)
      - [3.1. Purpose of Views](#31-purpose-of-views)
      - [3.2. Creating Views](#32-creating-views)
      - [3.3. Updatable Views](#33-updatable-views)
      - [3.4. Example](#34-example)
  - [4. Materialized Views](#4-materialized-views)
      - [4.1. Purpose of Materialized Views](#41-purpose-of-materialized-views)
      - [4.2. Creating Materialized Views](#42-creating-materialized-views)
      - [4.3. Refreshing Materialized Views](#43-refreshing-materialized-views)
      - [4.4. When to Use Materialized Views vs. Views](#44-when-to-use-materialized-views-vs-views)
      - [4.5. Example](#45-example)
  - [5. Data Types](#5-data-types)
      - [5.1. Choosing the Right Data Type](#51-choosing-the-right-data-type)
      - [5.2. Common Data Types](#52-common-data-types)
          - [5.2.1. Numeric Types](#521-numeric-types)
          - [5.2.2. String Types](#522-string-types)
          - [5.2.3. Date/Time Types](#523-date-time-types)
          - [5.2.4. JSON/JSONB Types](#524-jsonjsonb-types)
          - [5.2.5. Array Types](#525-array-types)
          - [5.2.6. Geometric Types](#526-geometric-types)
          - [5.2.7. Network Address Types](#527-network-address-types)
      - [5.3. Custom Data Types](#53-custom-data-types)
      - [5.4. Example](#54-example)
  - [6. Data Normalization](#6-data-normalization)
      - [6.1. Normalization Forms](#61-normalization-forms)
          - [6.1.1. 1NF (First Normal Form)](#611-1nf-first-normal-form)
          - [6.1.2. 2NF (Second Normal Form)](#612-2nf-second-normal-form)
          - [6.1.3. 3NF (Third Normal Form)](#613-3nf-third-normal-form)
          - [6.1.4. BCNF (Boyce-Codd Normal Form)](#614-bcnf-boyce-codd-normal-form)
      - [6.2. Benefits of Normalization](#62-benefits-of-normalization)
      - [6.3. Denormalization](#63-denormalization)
      - [6.4. Example](#64-example)
  - [7. Query Optimization](#7-query-optimization)
      - [7.1. Understanding Query Execution Plan](#71-understanding-query-execution-plan)
      - [7.2. Using EXPLAIN](#72-using-explain)
      - [7.3. Analyzing Slow Queries](#73-analyzing-slow-queries)
      - [7.4. Query Rewriting](#74-query-rewriting)
      - [7.5. Index Optimization](#75-index-optimization)
      - [7.6. Example](#76-example)
  - [Conclusion](#conclusion)

## Introduction <a name="introduction"></a>

Designing an efficient and maintainable database schema is crucial for the success of any application that relies on data persistence. PostgreSQL, as a powerful and feature-rich open-source relational database system, offers a wide array of tools and techniques for effective schema design and data modeling. This guide provides a detailed overview of best practices for designing PostgreSQL schemas, focusing on key aspects such as tables, indexes, views, materialized views, data types, data normalization, and query optimization. By adhering to these best practices, you can build robust, scalable, and performant PostgreSQL databases. This document aims to provide practical guidance and examples to help you master PostgreSQL schema design.

## 1\. Tables <a name="1-tables"></a>

Tables are the fundamental building blocks of any relational database. In PostgreSQL, designing tables effectively is the first step towards creating a well-structured database.

### 1.1. Naming Conventions <a name="11-naming-conventions"></a>

Consistent and meaningful naming conventions are essential for database maintainability and readability.

  * **Lowercase and Underscores:** Use lowercase letters and underscores to separate words in table names (e.g., `customer_orders`, `product_categories`). This convention improves readability and is widely adopted in PostgreSQL.
  * **Singular vs. Plural:** While there are debates, using plural names for tables (e.g., `customers`, `products`) is often preferred as tables typically store collections of entities. Choose a convention and stick to it consistently.
  * **Descriptive Names:** Table names should clearly indicate the data they store. Avoid abbreviations unless they are universally understood. For example, `user_profiles` is more descriptive than `usr_prf`.
  * **Avoid Reserved Words:** Do not use PostgreSQL reserved words (e.g., `user`, `order`, `group`) as table names. If you must use a reserved word, quote it (e.g., `"user"`), but it's better to find an alternative name.

### 1.2. Column Definition <a name="12-column-definition"></a>

Defining columns appropriately is crucial for data integrity and query performance.

  * **Choose the Right Data Type:** Select the most appropriate data type for each column to ensure data integrity and optimize storage. PostgreSQL offers a rich set of data types (discussed in detail in Section 5). For example, use `integer` for whole numbers, `numeric` for precise decimal numbers, `varchar` or `text` for strings, and `date` or `timestamp` for date and time values.
  * **NOT NULL Constraint:** Use `NOT NULL` constraints for columns that must always have a value. This enforces data integrity and can also improve query performance by allowing the query planner to make assumptions about the data.
  * **Default Values:** Define default values for columns when appropriate. This simplifies data insertion and ensures that a column has a reasonable value even if not explicitly provided during insertion.
  * **Column Comments:** Add comments to columns to explain their purpose and usage. Comments are invaluable for database documentation and understanding, especially in complex schemas.

<!-- end list -->

```sql
CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    product_name VARCHAR(255) NOT NULL,
    description TEXT,
    price NUMERIC(10, 2) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    category_id INTEGER REFERENCES categories(category_id),
    -- Product weight in kilograms
    weight_kg NUMERIC(5, 3)
);
COMMENT ON COLUMN products.weight_kg IS 'Product weight in kilograms';
```

### 1.3. Primary Keys <a name="13-primary-keys"></a>

Every table should have a primary key to uniquely identify each row.

  * **Uniqueness and NOT NULL:** Primary keys must be unique and not null. PostgreSQL automatically enforces these constraints.
  * **Single vs. Composite Primary Keys:** Choose a single-column primary key (e.g., `SERIAL`, `UUID`) whenever possible for simplicity and performance. Use composite primary keys only when necessary to represent natural keys that are composed of multiple columns.
  * **SERIAL Data Type:** For auto-incrementing integer primary keys, use the `SERIAL` data type. It automatically creates a sequence and sets the default value for the column to the next value from the sequence. For larger tables or distributed systems, consider `BIGSERIAL`.
  * **UUID Data Type:** For globally unique identifiers, use the `UUID` data type. UUIDs are useful when data is distributed across multiple databases or when you need to generate keys client-side.

<!-- end list -->

```sql
CREATE TABLE customers (
    customer_id SERIAL PRIMARY KEY,
    customer_uuid UUID UNIQUE NOT NULL DEFAULT gen_random_uuid(),
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL
);
```

### 1.4. Foreign Keys <a name="14-foreign-keys"></a>

Foreign keys establish relationships between tables, ensuring referential integrity.

  * **Referential Integrity:** Use foreign keys to link tables and enforce relationships. This ensures that data in related tables is consistent and prevents orphaned records.
  * **ON DELETE and ON UPDATE Actions:** Define appropriate `ON DELETE` and `ON UPDATE` actions for foreign keys. Common options include `CASCADE`, `SET NULL`, `RESTRICT`, and `NO ACTION`. Choose the action that best suits your application's data integrity requirements.
      * `CASCADE`: Delete or update related rows in the child table when the referenced row in the parent table is deleted or updated.
      * `SET NULL`: Set the foreign key column in the child table to `NULL` when the referenced row in the parent table is deleted or updated. Requires the foreign key column to be nullable.
      * `RESTRICT`: Prevent deletion or update of the referenced row in the parent table if related rows exist in the child table.
      * `NO ACTION`: Similar to `RESTRICT`, but the referential integrity check is performed after the statement is executed.
  * **Indexing Foreign Keys:** Ensure that foreign key columns are indexed. PostgreSQL does not automatically create indexes on foreign key columns, but indexing them is crucial for query performance, especially for joins.

<!-- end list -->

```sql
CREATE TABLE orders (
    order_id SERIAL PRIMARY KEY,
    customer_id INTEGER NOT NULL REFERENCES customers(customer_id) ON DELETE CASCADE,
    order_date DATE NOT NULL,
    total_amount NUMERIC(12, 2) NOT NULL
);

CREATE INDEX idx_orders_customer_id ON orders (customer_id);
```

### 1.5. Constraints <a name="15-constraints"></a>

Constraints enforce data integrity rules beyond data types and primary/foreign keys.

  * **CHECK Constraints:** Use `CHECK` constraints to enforce specific conditions on column values. This ensures that data conforms to business rules.
  * **UNIQUE Constraints:** Use `UNIQUE` constraints to ensure that values in a column or a set of columns are unique across all rows in a table.
  * **NOT NULL Constraints:** As mentioned earlier, use `NOT NULL` constraints to ensure that a column cannot contain null values.
  * **Example of CHECK and UNIQUE Constraints:**

<!-- end list -->

```sql
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    age INTEGER CHECK (age > 0)
);
```

### 1.6. Table Partitioning <a name="16-table-partitioning"></a>

For very large tables, consider table partitioning to improve query performance and manageability.

  * **Partitioning Methods:** PostgreSQL supports several partitioning methods:
      * **Range Partitioning:** Partition data based on ranges of column values (e.g., date ranges, numeric ranges).
      * **List Partitioning:** Partition data based on explicit lists of column values (e.g., regions, categories).
      * **Hash Partitioning:** Partition data based on a hash function of a column value, distributing data evenly across partitions.
  * **Benefits of Partitioning:**
      * **Improved Query Performance:** Queries can be directed to relevant partitions, reducing the amount of data scanned.
      * **Easier Data Management:** Partitioning simplifies tasks like data loading, archiving, and maintenance.
      * **Reduced Index Size:** Indexes are created on individual partitions, making them smaller and faster.
  * **Declarative Partitioning:** PostgreSQL 10 and later versions offer declarative partitioning, which simplifies the creation and management of partitioned tables.

<!-- end list -->

```sql
-- Range partitioning by order date
CREATE TABLE sales (
    sale_id SERIAL PRIMARY KEY,
    order_date DATE NOT NULL,
    region VARCHAR(50) NOT NULL,
    amount NUMERIC(10, 2) NOT NULL
) PARTITION BY RANGE (order_date);

CREATE TABLE sales_y2024m01 PARTITION OF sales FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
CREATE TABLE sales_y2024m02 PARTITION OF sales FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');
-- ... and so on for other months and years
```

### 1.7. Example <a name="17-example"></a>

Let's consider an example of designing tables for an e-commerce application.

```sql
-- Categories table
CREATE TABLE categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(255) UNIQUE NOT NULL,
    description TEXT
);

-- Products table
CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    product_name VARCHAR(255) NOT NULL,
    description TEXT,
    price NUMERIC(10, 2) NOT NULL,
    category_id INTEGER NOT NULL REFERENCES categories(category_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX idx_products_category_id ON products (category_id);

-- Customers table
CREATE TABLE customers (
    customer_id SERIAL PRIMARY KEY,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    address TEXT,
    phone_number VARCHAR(20)
);

-- Orders table
CREATE TABLE orders (
    order_id SERIAL PRIMARY KEY,
    customer_id INTEGER NOT NULL REFERENCES customers(customer_id),
    order_date DATE NOT NULL,
    total_amount NUMERIC(12, 2) NOT NULL,
    shipping_address TEXT,
    billing_address TEXT
);
CREATE INDEX idx_orders_customer_id ON orders (customer_id);

-- Order items table (many-to-many relationship between orders and products)
CREATE TABLE order_items (
    order_item_id SERIAL PRIMARY KEY,
    order_id INTEGER NOT NULL REFERENCES orders(order_id) ON DELETE CASCADE,
    product_id INTEGER NOT NULL REFERENCES products(product_id),
    quantity INTEGER NOT NULL CHECK (quantity > 0),
    price_per_unit NUMERIC(10, 2) NOT NULL
);
CREATE INDEX idx_order_items_order_id ON order_items (order_id);
CREATE INDEX idx_order_items_product_id ON order_items (product_id);

-- Add comments to tables and columns for documentation
COMMENT ON TABLE categories IS 'Product categories';
COMMENT ON COLUMN products.price IS 'Retail price of the product';
COMMENT ON TABLE orders IS 'Customer orders';
COMMENT ON TABLE order_items IS 'Items within each order';
```

## 2\. Indexes <a name="2-indexes"></a>

Indexes are crucial for improving query performance in PostgreSQL. They allow the database to quickly locate rows without scanning the entire table.

### 2.1. Index Types <a name="21-index-types"></a>

PostgreSQL offers various index types, each suited for different types of queries and data.

  * **B-tree Indexes:** The default and most common index type. B-tree indexes are efficient for equality and range queries, as well as sorted output. They work well for most data types.
    ```sql
    CREATE INDEX idx_customer_email ON customers USING btree (email);
    ```
  * **Hash Indexes:** Useful for equality lookups (using the `=` operator). Hash indexes are generally faster than B-tree indexes for equality checks but do not support range queries or ordered scans. They are less commonly used than B-tree indexes and are not crash-safe before PostgreSQL 10 (though this is largely historical now).
    ```sql
    CREATE INDEX idx_product_name_hash ON products USING hash (product_name);
    ```
  * **GiST (Generalized Search Tree) Indexes:** GiST indexes are highly versatile and support various indexing strategies, including R-tree indexes for spatial data and SP-GiST indexes for space-partitioned trees. They are used for indexing geometric data types, full-text search, and more.
    ```sql
    -- Example for geometric data
    CREATE INDEX idx_location_gist ON places USING gist (location);
    ```
  * **GIN (Generalized Inverted Index) Indexes:** GIN indexes are designed for indexing composite values, such as arrays and JSON documents. They are particularly effective for full-text search and when you need to search for values within arrays or JSON structures.
    ```sql
    -- Example for JSONB data
    CREATE INDEX idx_product_tags_gin ON products USING gin (tags);
    ```
  * **BRIN (Block Range Index) Indexes:** BRIN indexes are very space-efficient and are best suited for very large tables where certain columns are physically sorted or have a natural correlation with their physical order in the table (e.g., timestamp columns in time-series data). They are less effective for randomly ordered data.
    ```sql
    -- Example for time-series data
    CREATE INDEX idx_sensor_time_brin ON sensor_data USING brin (timestamp_column);
    ```

### 2.2. When to Use Indexes <a name="22-when-to-use-indexes"></a>

Indexes improve query performance but also add overhead to write operations (inserts, updates, deletes) and consume storage space. It's important to strategically choose when to create indexes.

  * **Columns in WHERE Clauses:** Index columns that are frequently used in `WHERE` clauses, especially in conditions involving `=`, `<`, `>`, `BETWEEN`, `IN`, and `LIKE` (for leading wildcards, e.g., `LIKE 'prefix%'`).
  * **Join Columns:** Index columns that are used in `JOIN` conditions. This significantly speeds up join operations. Foreign key columns are prime candidates for indexing.
  * **Columns in ORDER BY and GROUP BY Clauses:** Index columns used in `ORDER BY` and `GROUP BY` clauses can improve the performance of sorting and aggregation operations.
  * **Avoid Over-indexing:** Do not index every column. Over-indexing can degrade write performance and increase storage costs without providing significant query performance benefits. Index only columns that are frequently used in queries.
  * **Monitor Index Usage:** Use PostgreSQL tools like `pg_stat_statements` and `EXPLAIN` to monitor index usage and identify queries that could benefit from new indexes or index optimizations.

### 2.3. Composite Indexes <a name="23-composite-indexes"></a>

Composite indexes (indexes on multiple columns) can be very effective for queries that involve multiple columns in their `WHERE` clause.

  * **Order of Columns:** The order of columns in a composite index matters. The most frequently queried columns should come first. The index is most effective when queries filter on the leading columns of the index.
  * **Covering Indexes:** In some cases, a composite index can act as a covering index, meaning it includes all the columns needed to satisfy a query. This allows PostgreSQL to answer the query by only scanning the index, without accessing the table itself, which can be very fast (Index-Only Scans).

<!-- end list -->

```sql
-- Composite index on customer_id and order_date
CREATE INDEX idx_orders_customer_date ON orders (customer_id, order_date DESC);

-- Query that can benefit from this composite index
SELECT * FROM orders
WHERE customer_id = 123
ORDER BY order_date DESC
LIMIT 10;
```

### 2.4. Partial Indexes <a name="24-partial-indexes"></a>

Partial indexes are indexes built over a subset of rows in a table, defined by a `WHERE` clause. They are smaller and faster than full-table indexes when queries typically access only a portion of the table.

  * **Filtering Conditions:** Use partial indexes when queries frequently filter data based on specific conditions. For example, if you often query active users, you can create a partial index on the `is_active` column where `is_active` is true.
  * **Reduced Index Size:** Partial indexes are smaller, which reduces storage space and improves index scan performance.
  * **Example:**

<!-- end list -->

```sql
-- Partial index on active users
CREATE INDEX idx_active_users ON users (user_id, email)
WHERE is_active = TRUE;

-- Query that benefits from the partial index
SELECT user_id, email FROM users
WHERE is_active = TRUE
ORDER BY user_id;
```

### 2.5. Index Maintenance <a name="25-index-maintenance"></a>

Indexes need maintenance to remain efficient.

  * **Index Bloat:** Over time, indexes can become bloated due to updates and deletes, leading to decreased performance. Regularly monitor and rebuild indexes to reduce bloat. The `pg_repack` extension or `REINDEX` command can be used for rebuilding indexes.
  * **Statistics Collection:** PostgreSQL relies on statistics about table and index data to optimize query plans. Ensure that statistics are up-to-date by running `ANALYZE` regularly, especially after significant data changes.
  * **Unused Indexes:** Identify and remove unused indexes. Tools like `pgAdmin` or scripts using `pg_stat_user_indexes` can help identify indexes that are rarely or never used. Removing unused indexes reduces write overhead and storage consumption.

<!-- end list -->

```sql
-- Rebuild index to reduce bloat
REINDEX INDEX idx_orders_customer_date;

-- Analyze table to update statistics
ANALYZE orders;
```

### 2.6. Example <a name="26-example"></a>

Let's enhance the e-commerce database example with indexes.

```sql
-- Indexes for categories table (primary key index is automatically created)

-- Indexes for products table
CREATE INDEX idx_products_category_id ON products (category_id);
CREATE INDEX idx_products_name ON products (product_name); -- For product name searches

-- Indexes for customers table
CREATE INDEX idx_customers_email ON customers (email); -- For email lookups

-- Indexes for orders table
CREATE INDEX idx_orders_customer_id ON orders (customer_id);
CREATE INDEX idx_orders_order_date ON orders (order_date); -- For date-based order queries

-- Indexes for order_items table
CREATE INDEX idx_order_items_order_id ON order_items (order_id);
CREATE INDEX idx_order_items_product_id ON order_items (product_id);
```

## 3\. Views <a name="3-views"></a>

Views are virtual tables based on the result-set of an SQL statement. They do not store data themselves but provide a way to present data from one or more tables in a customized format.

### 3.1. Purpose of Views <a name="31-purpose-of-views"></a>

Views serve several important purposes in database design.

  * **Simplifying Complex Queries:** Views can encapsulate complex queries, making it easier for users to retrieve data without needing to understand the underlying table structure or complex joins.
  * **Data Abstraction:** Views provide a level of abstraction, hiding the complexity of the database schema from users. This allows schema changes to be made without affecting applications that rely on views (to a certain extent, if the view definition remains compatible).
  * **Security:** Views can restrict access to specific columns or rows of a table. By granting users access to views instead of base tables, you can control what data they can see and modify.
  * **Data Reorganization:** Views can reorganize data to present it in a more user-friendly or application-specific format. This can involve renaming columns, combining data from multiple tables, or calculating derived values.

### 3.2. Creating Views <a name="32-creating-views"></a>

Creating views in PostgreSQL is straightforward using the `CREATE VIEW` statement.

```sql
CREATE VIEW customer_order_summary AS
SELECT
    c.customer_id,
    c.first_name,
    c.last_name,
    c.email,
    o.order_id,
    o.order_date,
    o.total_amount
FROM
    customers c
JOIN
    orders o ON c.customer_id = o.customer_id;
```

This view `customer_order_summary` joins `customers` and `orders` tables and presents a simplified view of customer order information. Users can query this view as if it were a regular table.

```sql
SELECT * FROM customer_order_summary WHERE customer_id = 123;
```

### 3.3. Updatable Views <a name="33-updatable-views"></a>

Views can be updatable under certain conditions, allowing you to perform `INSERT`, `UPDATE`, and `DELETE` operations through the view.

  * **Simple Views:** Views are typically updatable if they are based on a single base table and do not involve aggregations, `DISTINCT`, `GROUP BY`, `HAVING`, or `UNION`.
  * **INSTEAD OF Triggers:** For more complex views, you can make them updatable by defining `INSTEAD OF` triggers. These triggers specify actions to be performed when an `INSERT`, `UPDATE`, or `DELETE` operation is attempted on the view.
  * **Example of Updatable View (Simple Case):**

<!-- end list -->

```sql
CREATE VIEW active_customers AS
SELECT customer_id, first_name, last_name, email
FROM customers
WHERE is_active = TRUE;

-- Inserting data through the view (will insert into the base table 'customers')
INSERT INTO active_customers (first_name, last_name, email)
VALUES ('John', 'Doe', '[email address removed]');

-- Updating data through the view
UPDATE active_customers SET last_name = 'Smith' WHERE email = '[email address removed]';

-- Deleting data through the view
DELETE FROM active_customers WHERE email = '[email address removed]';
```

  * **Example of INSTEAD OF Trigger for Complex View:**

<!-- end list -->

```sql
-- Non-updatable view (joins multiple tables)
CREATE VIEW order_details AS
SELECT
    o.order_id,
    c.first_name || ' ' || c.last_name AS customer_name,
    p.product_name,
    oi.quantity,
    oi.price_per_unit
FROM
    orders o
JOIN
    customers c ON o.customer_id = c.customer_id
JOIN
    order_items oi ON o.order_id = oi.order_id
JOIN
    products p ON oi.product_id = p.product_id;

-- Trigger to handle INSERT operations on order_details view
CREATE FUNCTION insert_order_details() RETURNS TRIGGER AS $$
BEGIN
    -- Complex logic to insert into base tables (orders, order_items, etc.)
    -- This is a simplified example and would need to be adapted to the specific view and table structure
    INSERT INTO orders (customer_id, order_date, total_amount) VALUES (...);
    INSERT INTO order_items (order_id, product_id, quantity, price_per_unit) VALUES (...);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER order_details_insert
INSTEAD OF INSERT ON order_details
FOR EACH ROW EXECUTE FUNCTION insert_order_details();
```

### 3.4. Example <a name="34-example"></a>

Let's create views for the e-commerce database.

```sql
-- View for product details with category name
CREATE VIEW product_details AS
SELECT
    p.product_id,
    p.product_name,
    p.description,
    p.price,
    c.category_name
FROM
    products p
JOIN
    categories c ON p.category_id = c.category_id;

-- View for customer order history summary
CREATE VIEW customer_order_history AS
SELECT
    c.customer_id,
    c.first_name,
    c.last_name,
    c.email,
    COUNT(o.order_id) AS total_orders,
    SUM(o.total_amount) AS total_spent,
    MAX(o.order_date) AS last_order_date
FROM
    customers c
LEFT JOIN
    orders o ON c.customer_id = o.customer_id
GROUP BY
    c.customer_id, c.first_name, c.last_name, c.email;

-- View for orders with customer and product details
CREATE VIEW full_order_details AS
SELECT
    o.order_id,
    o.order_date,
    o.total_amount,
    c.first_name || ' ' || c.last_name AS customer_name,
    c.email AS customer_email,
    p.product_name,
    oi.quantity,
    oi.price_per_unit
FROM
    orders o
JOIN
    customers c ON o.customer_id = c.customer_id
JOIN
    order_items oi ON o.order_id = oi.order_id
JOIN
    products p ON oi.product_id = p.product_id;
```

These views simplify querying product and order information, providing pre-joined and aggregated data for easier access and reporting.

## 4\. Materialized Views <a name="4-materialized-views"></a>

Materialized views are similar to views, but they store the result-set of the view's query as a physical table. This makes them faster to query than regular views, especially for complex and frequently accessed queries.

### 4.1. Purpose of Materialized Views <a name="41-purpose-of-materialized-views"></a>

Materialized views are used to improve the performance of read-heavy workloads, especially for complex aggregations or joins.

  * **Performance Improvement:** Materialized views pre-calculate and store query results, so querying a materialized view is much faster than re-executing the underlying query every time. This is particularly beneficial for dashboards, reports, and analytical queries that are run frequently.
  * **Data Snapshot:** Materialized views provide a snapshot of the data at the time of materialization. This can be useful for reporting on historical data or for maintaining a consistent view of data that changes frequently.
  * **Offloading Query Processing:** Materialized views can offload query processing from the main database tables, reducing the load on the transactional system. Complex queries are executed during materialization/refresh time, not during user query time.

### 4.2. Creating Materialized Views <a name="42-creating-materialized-views"></a>

Materialized views are created using the `CREATE MATERIALIZED VIEW` statement.

```sql
CREATE MATERIALIZED VIEW monthly_sales_summary AS
SELECT
    DATE_TRUNC('month', order_date) AS sales_month,
    SUM(total_amount) AS monthly_total_sales,
    COUNT(order_id) AS monthly_order_count
FROM
    orders
WHERE
    order_date >= DATE_TRUNC('year', CURRENT_DATE) - INTERVAL '1 year'
GROUP BY
    sales_month
ORDER BY
    sales_month;
```

This materialized view `monthly_sales_summary` calculates monthly sales totals and order counts for the past year.

### 4.3. Refreshing Materialized Views <a name="43-refreshing-materialized-views"></a>

Unlike regular views, materialized views need to be refreshed to update their data to reflect changes in the underlying tables.

  * **Manual Refresh:** Materialized views can be refreshed manually using the `REFRESH MATERIALIZED VIEW` command.
    ```sql
    REFRESH MATERIALIZED VIEW monthly_sales_summary;
    ```
  * **Concurrent Refresh:** For minimal downtime during refresh, use `REFRESH MATERIALIZED VIEW CONCURRENTLY`. This method allows concurrent reads while the view is being refreshed, but it requires a unique index on the materialized view and may take longer.
    ```sql
    CREATE UNIQUE INDEX idx_monthly_sales_summary_month ON monthly_sales_summary (sales_month);
    REFRESH MATERIALIZED VIEW CONCURRENTLY monthly_sales_summary;
    ```
  * **Scheduled Refresh:** Materialized views can be refreshed on a schedule using job schedulers like `pg_cron` or external scheduling tools. This keeps the materialized view data reasonably up-to-date.
  * **Trigger-Based Refresh:** For near real-time updates, you can use triggers on the base tables to automatically refresh the materialized view whenever data changes. However, this can add overhead to write operations.

<!-- end list -->

```sql
-- Example using pg_cron to refresh daily at midnight
-- Assuming pg_cron is installed and configured
SELECT cron.schedule('0 0 * * *', 'REFRESH MATERIALIZED VIEW CONCURRENTLY monthly_sales_summary');
```

### 4.4. When to Use Materialized Views vs. Views <a name="44-when-to-use-materialized-views-vs-views"></a>

Choosing between views and materialized views depends on the specific requirements of your application.

  * **Views:**
      * **Real-time Data:** Views always reflect the current data in the base tables.
      * **Low Write Load:** Suitable for applications with high write load and relatively low read frequency of complex queries.
      * **Less Storage:** Views do not consume additional storage space.
      * **Simpler to Manage:** No need for refresh operations.
  * **Materialized Views:**
      * **Performance for Read-Heavy Workloads:** Significantly faster for complex, frequently executed queries.
      * **Data Snapshots:** Useful for reporting and analytical queries where slightly stale data is acceptable.
      * **Higher Storage Cost:** Materialized views consume storage space to store the materialized data.
      * **Refresh Overhead:** Require periodic refresh operations, which can add overhead.
      * **Suitable Use Cases:** Dashboards, reports, summary tables, data warehousing, analytical queries.

### 4.5. Example <a name="45-example"></a>

Let's create materialized views for the e-commerce database to improve reporting performance.

```sql
-- Materialized view for daily sales summary
CREATE MATERIALIZED VIEW daily_sales_summary AS
SELECT
    order_date,
    SUM(total_amount) AS daily_total_sales,
    COUNT(order_id) AS daily_order_count
FROM
    orders
WHERE
    order_date >= CURRENT_DATE - INTERVAL '30 days' -- Last 30 days
GROUP BY
    order_date
ORDER BY
    order_date;

CREATE UNIQUE INDEX idx_daily_sales_summary_date ON daily_sales_summary (order_date);

-- Materialized view for product category sales summary
CREATE MATERIALIZED VIEW category_sales_summary AS
SELECT
    c.category_name,
    SUM(oi.quantity * oi.price_per_unit) AS category_total_sales
FROM
    order_items oi
JOIN
    products p ON oi.product_id = p.product_id
JOIN
    categories c ON p.category_id = c.category_id
GROUP BY
    c.category_name
ORDER BY
    category_total_sales DESC;

CREATE UNIQUE INDEX idx_category_sales_summary_category ON category_sales_summary (category_name);

-- Refresh materialized views (example for manual refresh)
REFRESH MATERIALIZED VIEW CONCURRENTLY daily_sales_summary;
REFRESH MATERIALIZED VIEW CONCURRENTLY category_sales_summary;
```

These materialized views provide pre-aggregated sales data, which can be queried quickly for dashboards and reports, without needing to perform complex aggregations on the base tables each time.

## 5\. Data Types <a name="5-data-types"></a>

Choosing the right data types for columns is crucial for data integrity, storage efficiency, and query performance in PostgreSQL. PostgreSQL offers a rich set of built-in data types, and also allows for the creation of custom data types.

### 5.1. Choosing the Right Data Type <a name="51-choosing-the-right-data-type"></a>

When selecting a data type for a column, consider the following factors:

  * **Data Integrity:** Choose a data type that accurately represents the kind of data you intend to store. For example, use numeric types for numbers, date/time types for dates and times, and string types for text.
  * **Storage Efficiency:** Select the smallest data type that can accommodate the expected range of values. Smaller data types consume less storage space and can improve query performance by reducing I/O. For example, use `smallint` instead of `integer` if the values are within the range of `smallint`.
  * **Query Performance:** Some data types are more efficient for certain operations. For example, integer types are generally faster for arithmetic operations and comparisons than text types.
  * **Application Requirements:** Consider the data types used in your application code. Matching database data types to application data types can simplify data mapping and reduce type conversion overhead.

### 5.2. Common Data Types <a name="52-common-data-types"></a>

PostgreSQL provides a wide variety of data types. Here are some of the most commonly used categories and examples:

#### 5.2.1. Numeric Types <a name="521-numeric-types"></a>

  * **`INTEGER` (or `INT`)**: Signed four-byte integer. Common choice for general-purpose integer columns.
      * Range: -2,147,483,648 to +2,147,483,647
  * **`SMALLINT`**: Signed two-byte integer. Use for smaller integer values to save space.
      * Range: -32,768 to +32,767
  * **`BIGINT`**: Signed eight-byte integer. Use for very large integers that exceed the range of `INTEGER`.
      * Range: -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807
  * **`NUMERIC` (or `DECIMAL`)**: Variable-precision decimal numbers. Use for exact numeric values, such as currency amounts, where precision is critical. You can specify precision and scale (e.g., `NUMERIC(10, 2)` for up to 10 digits with 2 decimal places).
  * **`REAL`**: Single precision floating-point number (four bytes). Use for approximate numeric values when storage space is a concern and precision is not critical.
  * **`DOUBLE PRECISION`**: Double precision floating-point number (eight bytes). Use for approximate numeric values when higher precision is needed than `REAL`.
  * **`SERIAL`, `BIGSERIAL`, `SMALLSERIAL`**: Not true data types, but shorthand for creating auto-incrementing integer columns. They automatically create sequences and set default values.

<!-- end list -->

```sql
CREATE TABLE numeric_examples (
    id SERIAL PRIMARY KEY,
    quantity SMALLINT,
    item_count INTEGER,
    large_value BIGINT,
    price NUMERIC(10, 2),
    approx_value REAL,
    precise_value DOUBLE PRECISION
);
```

#### 5.2.2. String Types <a name="522-string-types"></a>

  * **`VARCHAR(n)`**: Variable-length character string with a specified maximum length `n`. Use when you have a maximum length for strings and want to enforce it. Storage is optimized as it only uses space for the actual string plus a small overhead.
  * **`TEXT`**: Variable-length character string with no specified maximum length. Use for strings of arbitrary length. PostgreSQL does not enforce a maximum length for `TEXT` and it's generally preferred over `VARCHAR` unless you specifically need to enforce a length limit.
  * **`CHAR(n)`**: Fixed-length character string padded with spaces to length `n`. Less commonly used, typically for fixed-length codes or identifiers. If you use `CHAR` without length, it defaults to `CHAR(1)`.
  * **`CHARACTER VARYING(n)`**: Synonym for `VARCHAR(n)`.
  * **`CHARACTER(n)`**: Synonym for `CHAR(n)`.

<!-- end list -->

```sql
CREATE TABLE string_examples (
    id SERIAL PRIMARY KEY,
    short_string VARCHAR(50),
    long_text TEXT,
    fixed_code CHAR(10)
);
```

#### 5.2.3. Date/Time Types <a name="523-date-time-types"></a>

  * **`DATE`**: Date only (year, month, day). Use for storing dates without time components.
  * **`TIME [WITHOUT TIME ZONE]`**: Time of day only (hour, minute, second). Use for storing times without date or time zone information.
  * **`TIME WITH TIME ZONE`**: Time of day with time zone information. Stores time zone name or offset from UTC.
  * **`TIMESTAMP [WITHOUT TIME ZONE]`**: Date and time (year, month, day, hour, minute, second). Use for storing timestamps without time zone information. PostgreSQL assumes the local time zone if no time zone is specified.
  * **`TIMESTAMP WITH TIME ZONE` (or `TIMESTAMPTZ`)**: Date and time with time zone information. Stores timestamp values in UTC and converts to the specified time zone for display. Recommended for most applications to handle time zones correctly.
  * **`INTERVAL`**: Time duration. Use for storing periods of time.

<!-- end list -->

```sql
CREATE TABLE datetime_examples (
    id SERIAL PRIMARY KEY,
    event_date DATE,
    event_time_local TIME WITHOUT TIME ZONE,
    event_time_utc TIME WITH TIME ZONE,
    event_timestamp TIMESTAMP WITHOUT TIME ZONE,
    event_timestamp_tz TIMESTAMP WITH TIME ZONE,
    duration INTERVAL
);
```

#### 5.2.4. JSON/JSONB Types <a name="524-jsonjsonb-types"></a>

  * **`JSON`**: Stores JSON data as text. Preserves the exact formatting of the input JSON text, including whitespace and ordering of keys. Less efficient for querying.
  * **`JSONB`**: Stores JSON data in a binary format. Does not preserve whitespace or key order but is more efficient for querying and indexing. Recommended for most JSON use cases in PostgreSQL.

<!-- end list -->

```sql
CREATE TABLE json_examples (
    id SERIAL PRIMARY KEY,
    json_data JSON,
    jsonb_data JSONB
);

INSERT INTO json_examples (json_data, jsonb_data)
VALUES
    ('{"name": "Product", "price": 100}', '{"price": 100, "name": "Product"}');

-- Querying JSONB data (more efficient)
SELECT jsonb_data->>'name' AS product_name FROM json_examples;
```

#### 5.2.5. Array Types <a name="525-array-types"></a>

PostgreSQL supports arrays of any built-in or user-defined base type. Arrays are useful for storing lists of values within a single column.

```sql
CREATE TABLE array_examples (
    id SERIAL PRIMARY KEY,
    tags TEXT[],
    product_ids INTEGER[]
);

INSERT INTO array_examples (tags, product_ids)
VALUES
    (ARRAY['electronics', 'gadget', 'new'], ARRAY[1, 2, 3]);

-- Querying array data
SELECT tags[1] AS first_tag FROM array_examples;
SELECT * FROM array_examples WHERE 'gadget' = ANY(tags);
```

#### 5.2.6. Geometric Types <a name="526-geometric-types"></a>

PostgreSQL provides data types for representing geometric objects.

  * **`POINT`**: Represents a point in a 2D plane.
  * **`LINE`**: Represents an infinite line.
  * **`LSEG`**: Represents a line segment.
  * **`BOX`**: Represents a rectangular box.
  * **`PATH`**: Represents a sequence of connected points (can be open or closed).
  * **`POLYGON`**: Represents a closed path (polygon).
  * **`CIRCLE`**: Represents a circle.

<!-- end list -->

```sql
CREATE TABLE spatial_examples (
    id SERIAL PRIMARY KEY,
    location POINT,
    area POLYGON
);

INSERT INTO spatial_examples (location, area)
VALUES
    (POINT(10, 20), POLYGON '((0,0),(1,1),(1,0),(0,1))');

-- Spatial queries
SELECT * FROM spatial_examples WHERE location <@ BOX '((0,0),(20,30))'; -- Point within box
```

#### 5.2.7. Network Address Types <a name="527-network-address-types"></a>

  * **`INET`**: Represents an IPv4 or IPv6 host address, and optionally its subnet.
  * **`CIDR`**: Represents an IPv4 or IPv6 network address, specified according to RFC 1519 (Classless Inter-Domain Routing).
  * **`MACADDR`**: Represents a MAC (Media Access Control) address.
  * **`MACADDR8`**: Represents a MAC address in EUI-64 format (8 bytes).

<!-- end list -->

```sql
CREATE TABLE network_examples (
    id SERIAL PRIMARY KEY,
    ip_address INET,
    network CIDR,
    mac_address MACADDR
);

INSERT INTO network_examples (ip_address, network, mac_address)
VALUES
    ('192.168.1.10/24', '192.168.1.0/24', '08:00:2b:01:02:03');

-- Network address queries
SELECT * FROM network_examples WHERE ip_address << '192.168.1.0/24'; -- IP address is within network
```

### 5.3. Custom Data Types <a name="53-custom-data-types"></a>

PostgreSQL allows you to define your own custom data types, which can be useful for representing domain-specific data.

  * **Composite Types:** Define structured types with multiple attributes.
    ```sql
    CREATE TYPE address AS (
        street VARCHAR(200),
        city VARCHAR(100),
        postal_code VARCHAR(20),
        country VARCHAR(50)
    );

    CREATE TABLE employees (
        employee_id SERIAL PRIMARY KEY,
        name VARCHAR(200),
        home_address ADDRESS
    );

    INSERT INTO employees (name, home_address)
    VALUES ('Alice', ROW('123 Main St', 'Anytown', '12345', 'USA')::ADDRESS);

    SELECT (home_address).city FROM employees WHERE name = 'Alice';
    ```
  * **Enum Types:** Define enumerated types with a fixed set of possible values.
    ```sql
    CREATE TYPE order_status AS ENUM ('PENDING', 'PROCESSING', 'SHIPPED', 'DELIVERED', 'CANCELLED');

    CREATE TABLE order_status_examples (
        order_id SERIAL PRIMARY KEY,
        status ORDER_STATUS DEFAULT 'PENDING'
    );

    INSERT INTO order_status_examples (status) VALUES ('SHIPPED'), ('CANCELLED');
    SELECT * FROM order_status_examples WHERE status = 'SHIPPED';
    ```
  * **Domain Types:** Create constraints on existing data types.
    ```sql
    CREATE DOMAIN email_address AS TEXT
    CHECK (VALUE ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$');

    CREATE TABLE email_example (
        id SERIAL PRIMARY KEY,
        email EMAIL_ADDRESS
    );

    INSERT INTO email_example (email) VALUES ('[email address removed]'); -- Works
    INSERT INTO email_example (email) VALUES ('invalid-email'); -- Fails CHECK constraint
    ```

### 5.4. Example <a name="54-example"></a>

Let's refine the e-commerce database schema by using appropriate data types.

```sql
-- Categories table
CREATE TABLE categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(255) UNIQUE NOT NULL,
    description TEXT
);

-- Products table
CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    product_name VARCHAR(255) NOT NULL,
    description TEXT,
    price NUMERIC(10, 2) NOT NULL,
    category_id INTEGER NOT NULL REFERENCES categories(category_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    is_featured BOOLEAN DEFAULT FALSE, -- Boolean type for flags
    tags TEXT[] -- Array of tags for products
);
CREATE INDEX idx_products_category_id ON products (category_id);

-- Customers table
CREATE TABLE customers (
    customer_id SERIAL PRIMARY KEY,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    address TEXT,
    phone_number VARCHAR(20),
    registration_date DATE DEFAULT CURRENT_DATE -- DATE type for registration date
);

-- Orders table
CREATE TABLE orders (
    order_id SERIAL PRIMARY KEY,
    customer_id INTEGER NOT NULL REFERENCES customers(customer_id),
    order_date DATE NOT NULL,
    total_amount NUMERIC(12, 2) NOT NULL,
    shipping_address TEXT,
    billing_address TEXT,
    order_timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP -- TIMESTAMP WITH TIME ZONE for order timestamp
);
CREATE INDEX idx_orders_customer_id ON orders (customer_id);

-- Order items table
CREATE TABLE order_items (
    order_item_id SERIAL PRIMARY KEY,
    order_id INTEGER NOT NULL REFERENCES orders(order_id) ON DELETE CASCADE,
    product_id INTEGER NOT NULL REFERENCES products(product_id),
    quantity SMALLINT NOT NULL CHECK (quantity > 0), -- SMALLINT for quantity
    price_per_unit NUMERIC(10, 2) NOT NULL
);
CREATE INDEX idx_order_items_order_id ON order_items (order_id);
CREATE INDEX idx_order_items_product_id ON order_items (product_id);
```

By carefully selecting data types, we ensure data integrity, optimize storage, and improve query performance in our e-commerce database.

## 6\. Data Normalization <a name="6-data-normalization"></a>

Data normalization is the process of organizing data in a database to minimize redundancy and dependency by dividing larger tables into smaller tables and defining relationships between them. Normalization aims to reduce data anomalies and improve data integrity.

### 6.1. Normalization Forms <a name="61-normalization-forms"></a>

Normalization is typically described in terms of normal forms. The most common normal forms are 1NF, 2NF, 3NF, and BCNF (Boyce-Codd Normal Form). Higher normal forms generally provide better data integrity but can sometimes lead to more complex queries and potentially reduced query performance due to increased joins.

#### 6.1.1. 1NF (First Normal Form) <a name="611-1nf-first-normal-form"></a>

A table is in 1NF if it meets the following criteria:

  * **Atomic Values:** Each column in a row contains only atomic (indivisible) values. No multi-valued attributes or repeating groups are allowed.
  * **No Repeating Groups:** There are no repeating groups of columns within a table.

**Example of a table not in 1NF:**

```
Table: Orders_Not_1NF
--------------------------------------------------
| OrderID | CustomerName | ProductIDs | ProductNames        |
|---------|--------------|------------|---------------------|
| 1       | John Doe     | 1, 2       | ProductA, ProductB  |
| 2       | Jane Smith   | 3          | ProductC            |
--------------------------------------------------
```

In this example, `ProductIDs` and `ProductNames` are multi-valued attributes, violating 1NF.

**Table in 1NF:**

```
Table: Orders_1NF
--------------------------------------------------
| OrderID | CustomerName | ProductID | ProductName |
|---------|--------------|-----------|-------------|
| 1       | John Doe     | 1         | ProductA    |
| 1       | John Doe     | 2         | ProductB    |
| 2       | Jane Smith   | 3         | ProductC    |
--------------------------------------------------
```

In the 1NF version, each row represents a single product in an order, and all values are atomic.

#### 6.1.2. 2NF (Second Normal Form) <a name="612-2nf-second-normal-form"></a>

A table is in 2NF if it meets the following criteria:

  * **It is in 1NF.**
  * **Full Functional Dependency:** All non-key attributes are fully functionally dependent on the entire primary key. This means that if the primary key is composite, every non-key attribute must depend on all components of the primary key, not just part of it.

**Example of a table in 1NF but not in 2NF:**

```
Table: OrderDetails_1NF
-----------------------------------------------------------------
| OrderID | ProductID | ProductName | OrderDate  | CustomerName |
|---------|-----------|-------------|------------|--------------|
| 1       | 1         | ProductA    | 2024-01-01 | John Doe     |
| 1       | 2         | ProductB    | 2024-01-01 | John Doe     |
| 2       | 3         | ProductC    | 2024-01-05 | Jane Smith   |
-----------------------------------------------------------------
Primary Key: (OrderID, ProductID)
```

In this table, `ProductName` depends only on `ProductID`, and `OrderDate` and `CustomerName` depend only on `OrderID`. This violates 2NF because non-key attributes are not fully dependent on the composite primary key.

**Tables in 2NF:**

```
Table: OrderItems_2NF
---------------------------------------
| OrderID | ProductID | ProductName |
|---------|-----------|-------------|
| 1       | 1         | ProductA    |
| 1       | 2         | ProductB    |
| 2       | 3         | ProductC    |
---------------------------------------
Primary Key: (OrderID, ProductID)
Foreign Key: ProductID -> Products(ProductID)

Table: Orders_2NF
-------------------------------------
| OrderID | OrderDate  | CustomerName |
|---------|------------|--------------|
| 1       | 2024-01-01 | John Doe     |
| 2       | 2024-01-05 | Jane Smith   |
-------------------------------------
Primary Key: OrderID
```

Now, `OrderItems_2NF` and `Orders_2NF` are in 2NF. `ProductName` is moved to a `Products` table and depends only on `ProductID`. `OrderDate` and `CustomerName` depend only on `OrderID`.

#### 6.1.3. 3NF (Third Normal Form) <a name="613-3nf-third-normal-form"></a>

A table is in 3NF if it meets the following criteria:

  * **It is in 2NF.**
  * **No Transitive Dependency:** No non-key attribute is transitively dependent on the primary key. This means that non-key attributes should depend directly on the primary key and not indirectly through another non-key attribute.

**Example of a table in 2NF but not in 3NF:**

```
Table: Orders_2NF_Not_3NF
-----------------------------------------------------
| OrderID | CustomerID | CustomerName | CustomerCity |
|---------|------------|--------------|--------------|
| 1       | 101        | John Doe     | Anytown      |
| 2       | 102        | Jane Smith   | Otherville   |
| 3       | 101        | John Doe     | Anytown      |
-----------------------------------------------------
Primary Key: OrderID
Foreign Key: CustomerID -> Customers(CustomerID)
```

In this table, `CustomerCity` is transitively dependent on `OrderID` through `CustomerID`. `OrderID` -\> `CustomerID` -\> `CustomerCity`. This violates 3NF.

**Tables in 3NF:**

```
Table: Orders_3NF
-------------------------------------
| OrderID | CustomerID | OrderDate  |
|---------|------------|------------|
| 1       | 101        | 2024-01-01 |
| 2       | 102        | 2024-01-05 |
| 3       | 101        | 2024-01-07 |
-------------------------------------
Primary Key: OrderID
Foreign Key: CustomerID -> Customers(CustomerID)

Table: Customers_3NF
-------------------------------------
| CustomerID | CustomerName | CustomerCity |
|------------|--------------|--------------|
| 101        | John Doe     | Anytown      |
| 102        | Jane Smith   | Otherville   |
-------------------------------------
Primary Key: CustomerID
```

Now, `Orders_3NF` and `Customers_3NF` are in 3NF. `CustomerCity` is moved to the `Customers` table and depends directly on `CustomerID`.

#### 6.1.4. BCNF (Boyce-Codd Normal Form) <a name="614-bcnf-boyce-codd-normal-form"></a>

BCNF is a stricter version of 3NF. A table is in BCNF if it meets the following criteria:

  * **It is in 3NF.**
  * **For every functional dependency X  Y, X must be a superkey.** In simpler terms, every determinant must be a candidate key. BCNF addresses cases where there are multiple candidate keys in a table.

BCNF is typically relevant when a table has composite candidate keys that overlap. In most practical scenarios, achieving 3NF is sufficient. BCNF is mainly considered in more complex database designs.

### 6.2. Benefits of Normalization <a name="62-benefits-of-normalization"></a>

  * **Reduced Data Redundancy:** Normalization minimizes data duplication, saving storage space and reducing inconsistencies.
  * **Improved Data Integrity:** By reducing redundancy and enforcing dependencies, normalization helps maintain data consistency and accuracy.
  * **Simplified Data Maintenance:** Updates, inserts, and deletes become simpler and less error-prone because data is stored in a structured and non-redundant manner.
  * **Better Data Organization:** Normalized databases are generally better organized and easier to understand and query.

### 6.3. Denormalization <a name="63-denormalization"></a>

While normalization is generally beneficial, there are situations where denormalization (intentionally introducing redundancy) can be advantageous.

  * **Performance Improvement for Read-Heavy Workloads:** Denormalization can reduce the number of joins required for common queries, improving read performance. This is often used in data warehouses or reporting databases where read performance is critical.
  * **Simplified Queries:** Denormalized schemas can simplify certain queries, making them easier to write and understand.
  * **Trade-offs:** Denormalization introduces redundancy, which can increase storage space and potentially lead to data anomalies if not managed carefully. It should be applied judiciously and only when performance gains outweigh the potential drawbacks.
  * **Common Denormalization Techniques:**
      * **Adding Redundant Columns:** Duplicating columns from related tables into a table to avoid joins.
      * **Creating Summary Tables:** Pre-calculating and storing aggregated data (similar to materialized views, but can be done in regular tables).
      * **Joining Tables into Wide Tables:** Combining multiple tables into a single wide table, often used in data warehouses.

### 6.4. Example <a name="64-example"></a>

Let's consider normalization for the e-commerce database. The schema we designed in previous sections is already largely normalized up to 3NF. We have separated data into tables like `categories`, `products`, `customers`, `orders`, and `order_items`, and established relationships using foreign keys.

**Example of Denormalization (for reporting performance):**

Suppose we frequently need to generate reports that include product category name along with order item details. Joining `order_items`, `products`, and `categories` tables for every report query can be performance-intensive for large datasets. We might consider denormalizing by adding `category_name` directly to the `order_items` table (or a reporting-specific table).

```sql
-- Original Normalized Schema (Order Items Table)
CREATE TABLE order_items (
    order_item_id SERIAL PRIMARY KEY,
    order_id INTEGER NOT NULL REFERENCES orders(order_id) ON DELETE CASCADE,
    product_id INTEGER NOT NULL REFERENCES products(product_id),
    quantity SMALLINT NOT NULL CHECK (quantity > 0),
    price_per_unit NUMERIC(10, 2) NOT NULL
);

-- Denormalized Schema (Reporting Order Items Table)
CREATE TABLE reporting_order_items AS
SELECT
    oi.order_item_id,
    oi.order_id,
    oi.product_id,
    p.product_name,
    c.category_name, -- Denormalized column
    oi.quantity,
    oi.price_per_unit
FROM
    order_items oi
JOIN
    products p ON oi.product_id = p.product_id
JOIN
    categories c ON p.category_id = c.category_id;

CREATE INDEX idx_reporting_order_items_order_id ON reporting_order_items (order_id);
CREATE INDEX idx_reporting_order_items_product_id ON reporting_order_items (product_id);
```

In this denormalized `reporting_order_items` table, we've added `category_name` directly. Reports querying this table will no longer need to join with the `categories` table, potentially improving query performance for reporting purposes. However, we've introduced redundancy, and updates to category names would need to be reflected in this denormalized table as well (which could be handled by triggers or ETL processes).

## 7\. Query Optimization <a name="7-query-optimization"></a>

Query optimization is the process of improving the performance of SQL queries. Efficiently written queries are crucial for database performance, especially as data volumes grow.

### 7.1. Understanding Query Execution Plan <a name="71-understanding-query-execution-plan"></a>

PostgreSQL's query optimizer generates an execution plan for each query, detailing how the database will retrieve and process the data. Understanding query execution plans is essential for identifying performance bottlenecks and optimizing queries.

  * **`EXPLAIN` Command:** Use the `EXPLAIN` command to view the execution plan for a query.
    ```sql
    EXPLAIN SELECT * FROM orders WHERE customer_id = 123 AND order_date >= '2024-01-01';
    ```
  * **Plan Components:** Execution plans typically include operations like:
      * **Seq Scan:** Sequential scan of a table (reads every row). Generally slow for large tables.
      * **Index Scan:** Uses an index to locate rows. Much faster than sequential scan for indexed columns.
      * **Index Only Scan:** Retrieves data directly from the index without accessing the table (fastest if index covers all needed columns).
      * **Bitmap Heap Scan:** Uses a bitmap index to locate rows, followed by accessing the table heap to retrieve the actual data.
      * **Bitmap Index Scan:** Scans a bitmap index to create a bitmap of rows to be accessed.
      * **Nested Loop Join:** Joins tables by iterating through each row of the outer table and finding matching rows in the inner table. Can be inefficient for large tables without proper indexes.
      * **Hash Join:** Builds a hash table of one table and probes it with rows from the other table. Efficient for large tables when memory is sufficient.
      * **Merge Join:** Joins sorted tables by merging them. Efficient if tables are already sorted or can be sorted efficiently.
      * **Sort:** Sorts the result set. Can be expensive for large datasets.
      * **Aggregate:** Performs aggregation operations (e.g., `SUM`, `COUNT`, `AVG`).
  * **`EXPLAIN ANALYZE`:** Use `EXPLAIN ANALYZE` to not only view the execution plan but also execute the query and get actual execution times for each step. This is useful for identifying real performance bottlenecks. Be cautious when using `EXPLAIN ANALYZE` on `UPDATE`, `INSERT`, or `DELETE` statements in production as it will execute the query.

<!-- end list -->

```sql
EXPLAIN ANALYZE SELECT * FROM orders WHERE customer_id = 123 AND order_date >= '2024-01-01';
```

### 7.2. Using EXPLAIN <a name="72-using-explain"></a>

Analyzing `EXPLAIN` output helps identify areas for query optimization. Look for:

  * **Sequential Scans on Large Tables:** Sequential scans (`Seq Scan`) on large tables are often a performance bottleneck. Try to replace them with index scans by adding appropriate indexes or rewriting queries to use existing indexes.
  * **Inefficient Joins:** Nested loop joins (`Nested Loop`) can be slow for large tables. Hash joins (`Hash Join`) or merge joins (`Merge Join`) are often more efficient, especially when appropriate indexes are in place. Ensure join columns are indexed.
  * **Expensive Sort Operations:** Sort operations (`Sort`) can be costly, especially for large datasets. Try to avoid sorting if possible, or optimize sorting by using indexes or rewriting queries.
  * **Bitmap Scans:** Bitmap scans are generally better than sequential scans but can sometimes be less efficient than index-only scans or direct index scans. Review if indexes can be optimized to enable index-only scans.
  * **Filter Conditions:** Check if filter conditions (`WHERE` clauses) are effectively used to reduce the number of rows processed early in the execution plan. Ensure that filter columns are indexed.

### 7.3. Analyzing Slow Queries <a name="73-analyzing-slow-queries"></a>

Identify slow queries using PostgreSQL's logging and monitoring tools.

  * **`pg_stat_statements` Extension:** Install and enable the `pg_stat_statements` extension to track execution statistics for all SQL statements executed by the server. It provides valuable information about query execution time, frequency, and resource usage.
    ```sql
    CREATE EXTENSION pg_stat_statements;
    SELECT * FROM pg_stat_statements ORDER BY mean_time DESC LIMIT 10; -- Top 10 slowest queries
    ```
  * **PostgreSQL Logs:** Configure PostgreSQL logging to capture slow queries. Set `log_min_duration_statement` in `postgresql.conf` to log queries that take longer than a specified duration.
    ```
    log_min_duration_statement = 250ms  -- Log queries that take 250ms or more
    ```
    Analyze the logs to identify slow-running queries and their patterns.
  * **Performance Monitoring Tools:** Use PostgreSQL monitoring tools (e.g., `pgAdmin`, `Prometheus`, `Grafana`, cloud provider monitoring tools) to visualize database performance metrics and identify slow queries and performance bottlenecks.

### 7.4. Query Rewriting <a name="74-query-rewriting"></a>

Sometimes, rewriting a query can significantly improve its performance without changing the schema or adding indexes.

  * **Avoid `SELECT *`:** Instead of `SELECT *`, explicitly list only the columns you need. This reduces I/O and network traffic, especially for tables with many columns or large data types.
  * **Use `JOIN`s Efficiently:** Choose the appropriate join type (`INNER JOIN`, `LEFT JOIN`, etc.) and ensure join conditions are properly indexed. In some cases, rewriting joins or using subqueries can improve performance.
  * **Optimize `WHERE` Clauses:** Place the most selective conditions in the `WHERE` clause first to reduce the number of rows processed early. Use indexes effectively in `WHERE` conditions.
  * **Limit Results with `LIMIT`:** If you only need a subset of rows, use `LIMIT` to restrict the number of rows returned. This is particularly useful for pagination or fetching top N results.
  * **Use Aggregation Carefully:** Aggregation operations (`GROUP BY`, `SUM`, `AVG`, etc.) can be expensive. Optimize aggregation queries by using indexes, materialized views, or pre-aggregated tables if possible.
  * **Avoid Functions in `WHERE` Clauses:** Using functions in `WHERE` clauses can prevent index usage. If possible, rewrite queries to avoid functions or use functional indexes if needed.
  * **Example of Query Rewriting:**

**Original Query (inefficient `SELECT *` and potential sequential scan):**

```sql
SELECT * FROM orders
WHERE order_date BETWEEN '2024-01-01' AND '2024-01-31'
ORDER BY customer_id;
```

**Rewritten Query (explicit columns, index usage, limit results if needed):**

```sql
SELECT order_id, customer_id, order_date, total_amount -- Explicit columns
FROM orders
WHERE order_date >= '2024-01-01' AND order_date <= '2024-01-31' -- Index-friendly range condition
ORDER BY customer_id
LIMIT 1000; -- Limit results if only a subset is needed
```

### 7.5. Index Optimization <a name="75-index-optimization"></a>

Index optimization is a key aspect of query optimization.

  * **Create Indexes for Common Query Patterns:** Analyze query patterns and create indexes that match the columns used in `WHERE`, `JOIN`, `ORDER BY`, and `GROUP BY` clauses.
  * **Choose the Right Index Type:** Select the appropriate index type (B-tree, Hash, GiST, GIN, BRIN) based on the query types and data characteristics (as discussed in Section 2.1).
  * **Composite Indexes for Multi-Column Queries:** Use composite indexes for queries that filter or join on multiple columns. Consider the column order in composite indexes.
  * **Partial Indexes for Filtered Data:** Use partial indexes when queries frequently access a subset of data based on filter conditions.
  * **Index Maintenance:** Regularly rebuild indexes to reduce bloat and update statistics using `REINDEX` and `ANALYZE` (as discussed in Section 2.5).
  * **Monitor Index Usage:** Use `pg_stat_statements` and `EXPLAIN` to monitor index usage and identify unused or inefficient indexes.

### 7.6. Example <a name="76-example"></a>

Let's optimize queries in the e-commerce database.

**Scenario: Slow query to find orders placed by customers from a specific city in a date range.**

```sql
-- Slow Query (example)
SELECT
    o.order_id,
    c.first_name,
    c.last_name,
    c.address,
    o.order_date,
    o.total_amount
FROM
    orders o
JOIN
    customers c ON o.customer_id = c.customer_id
WHERE
    c.address LIKE '%Leiria%' -- Searching address for city
    AND o.order_date BETWEEN '2024-01-01' AND '2024-03-31';
```

**Optimization Steps:**

1.  **Analyze Execution Plan:** Use `EXPLAIN ANALYZE` to see the execution plan. It likely shows sequential scans and inefficient joins.
2.  **Add Indexes:**
      * Index on `customers.address` for city search (consider full-text search or specialized indexing if address search is frequent and complex). For simple `LIKE '%Leiria%'`, a B-tree index on `address` might help.
      * Index on `orders.order_date` for date range queries.
      * Ensure `orders.customer_id` and `customers.customer_id` are indexed (foreign key indexes should already exist).
3.  **Rewrite Query (if needed):**
      * If `LIKE '%Leiria%'` is very slow, consider normalizing address into separate columns (city, street, etc.) and indexing `city`.
      * Ensure explicit column selection instead of `SELECT *`.

**Optimized Schema and Indexes:**

```sql
-- Add index on customers.address (or consider normalizing address)
CREATE INDEX idx_customers_address ON customers (address);
-- Indexes on orders.customer_id and orders.order_date already exist

-- Optimized Query
SELECT
    o.order_id,
    c.first_name,
    c.last_name,
    c.address,
    o.order_date,
    o.total_amount
FROM
    orders o
JOIN
    customers c ON o.customer_id = c.customer_id
WHERE
    c.address LIKE '%Leiria%'
    AND o.order_date BETWEEN '2024-01-01' AND '2024-03-31';
```

After adding indexes and potentially rewriting the query, re-run `EXPLAIN ANALYZE` to verify that the execution plan has improved and that the query is now faster.

## Conclusion <a name="conclusion"></a>

Effective PostgreSQL schema design and data modeling are critical for building robust, scalable, and performant database applications. By following best practices for tables, indexes, views, materialized views, data types, data normalization, and query optimization, you can create databases that are efficient, maintainable, and well-suited to your application's needs. Remember that database design is an iterative process. Continuously monitor, analyze, and refine your schema and queries as your application evolves and data volumes grow. Regularly review execution plans, identify slow queries, and adjust indexes and queries as needed to maintain optimal performance. By mastering these principles and techniques, you can leverage the full power of PostgreSQL to build high-quality data-driven applications.

```
