# Disclaimer
This repository contains information collected from various online sources and/or generated by AI assistants. The content provided here is for informational purposes only and is intended to serve as a general reference on various topics.

# PostgreSQL Administration Guide

## Table of Contents
- [Best Practices for PostgreSQL Administration](#best-practices-for-postgresql-administration)
  - [Installation and Setup](#installation-and-setup)
  - [Configuration Management](#configuration-management)
  - [Backup and Recovery Strategies](#backup-and-recovery-strategies)
  - [Security Best Practices](#security-best-practices)
  - [Database Maintenance](#database-maintenance)
  - [High Availability and Replication](#high-availability-and-replication)
- [Performance Tuning and Optimization](#performance-tuning-and-optimization)
  - [System Resource Management](#system-resource-management)
  - [Memory Configuration](#memory-configuration)
  - [Query Optimization](#query-optimization)
  - [Indexing Strategies](#indexing-strategies)
  - [VACUUM and ANALYZE](#vacuum-and-analyze)
  - [Connection Pooling](#connection-pooling)
  - [Monitoring Performance](#monitoring-performance)
- [Logging, Auditing, and Troubleshooting](#logging-auditing-and-troubleshooting)
  - [Logging Configuration](#logging-configuration)
  - [Log Analysis](#log-analysis)
  - [Auditing Database Activities](#auditing-database-activities)
  - [Common Issues and Solutions](#common-issues-and-solutions)
  - [Diagnostic Tools](#diagnostic-tools)
  - [Performance Troubleshooting](#performance-troubleshooting)

## Best Practices for PostgreSQL Administration

### Installation and Setup

#### Proper Installation
- Use official package repositories when possible
- Consider using containers (Docker) for development and testing
- Separate data directories from installation directories
- Install from source only when necessary (specific features/versions)

#### Initial Database Setup
```bash
# Initialize database cluster with specified locale and encoding
initdb -D /path/to/data --locale=en_US.UTF-8 --encoding=UTF8

# Create a dedicated database user for application
createuser --pwprompt app_user

# Create application database with proper ownership
createdb --owner=app_user app_database
```

#### Version Selection Considerations
- **Long-Term Support (LTS)**: Choose stable versions for production
- **End-of-Life (EOL)**: Plan upgrades before your PostgreSQL version reaches EOL
- **Feature Requirements**: Balance new features against stability needs

### Configuration Management

#### Critical Configuration Files
- **postgresql.conf**: Main server configuration
- **pg_hba.conf**: Client authentication control
- **pg_ident.conf**: User name mapping

#### Configuration Best Practices
- Document all configuration changes
- Use a version control system for configuration files
- Implement configuration as code when possible
- Test configuration changes in development before production

#### Essential Configuration Parameters
```
# Memory settings
shared_buffers = 25% of RAM (up to 8GB)
work_mem = 4MB-64MB (depends on workload)
maintenance_work_mem = 64MB-1GB

# Write Ahead Log (WAL)
wal_level = replica (minimum for replication)
max_wal_size = 1GB-16GB
checkpoint_timeout = 5min-30min

# Connection settings
max_connections = 100-500 (workload dependent)
superuser_reserved_connections = 3-10

# Query Planning
random_page_cost = 1.1-3.0 (depends on storage)
effective_cache_size = 50-75% of RAM

# Logging (basic)
log_destination = 'csvlog'
logging_collector = on
log_directory = 'pg_log'
```

#### Sample postgresql.conf for Medium-Size Database
```
# Connection settings
listen_addresses = '*'
max_connections = 200
superuser_reserved_connections = 5

# Memory settings
shared_buffers = 2GB
work_mem = 16MB
maintenance_work_mem = 256MB
effective_cache_size = 6GB

# Write-Ahead Log
wal_level = replica
max_wal_size = 4GB
min_wal_size = 1GB
checkpoint_timeout = 15min

# Query Planning
random_page_cost = 1.1  # For SSD storage
effective_io_concurrency = 200  # For SSD storage

# Background Writer
bgwriter_delay = 200ms
bgwriter_lru_maxpages = 100
bgwriter_lru_multiplier = 2.0
```

### Backup and Recovery Strategies

#### Logical Backups with pg_dump
```bash
# Simple database backup
pg_dump dbname > backup.sql

# Custom format (compressed, parallel, most flexible)
pg_dump -Fc -j 4 dbname > backup.dump

# Directory format (for largest databases with parallelism)
pg_dump -Fd -j 4 dbname -f backup_dir/
```

#### Physical Backups
```bash
# Using pg_basebackup
pg_basebackup -D /backup/path -Ft -z -P -X stream

# Continuous archiving setup in postgresql.conf
archive_mode = on
archive_command = 'cp %p /archive/path/%f'
```

#### Recovery Planning
- Document recovery time objectives (RTO) and recovery point objectives (RPO)
- Test recovery procedures regularly
- Implement point-in-time recovery (PITR) for critical systems
- Store backups offsite or in a different availability zone

#### Backup Automation Script Example
```bash
#!/bin/bash
# PostgreSQL backup script

BACKUP_DIR="/var/lib/postgresql/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DB_NAME="production_db"
RETENTION_DAYS=7

# Create backup directory structure
mkdir -p $BACKUP_DIR/daily

# Run backup
pg_dump -Fc -j 4 $DB_NAME > $BACKUP_DIR/daily/backup_${DB_NAME}_${TIMESTAMP}.dump

# Delete old backups
find $BACKUP_DIR/daily -name "backup_${DB_NAME}_*" -type f -mtime +$RETENTION_DAYS -delete
```

### Security Best Practices

#### Authentication Control
- Use strong password policies
- Implement SCRAM authentication (scram-sha-256)
- Consider certificate-based authentication for sensitive systems
- Limit superuser access

#### Network Security
- Restrict listen_addresses to necessary interfaces
- Use SSL/TLS for all connections
- Configure pg_hba.conf to be restrictive by default

#### Sample pg_hba.conf with Security Focus
```
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             postgres                                peer
local   all             all                                     scram-sha-256
host    all             all             127.0.0.1/32            scram-sha-256
host    all             all             ::1/128                 scram-sha-256
host    replication     replication     10.0.0.0/24             scram-sha-256
hostssl all             all             0.0.0.0/0               scram-sha-256
```

#### Data Encryption
- Enable SSL/TLS for transport encryption
- Use encrypted partitions for data at rest protection
- Consider column-level encryption for sensitive data

#### Role and Permission Management
- Follow principle of least privilege
- Create application-specific roles
- Use row-level security for multi-tenant databases
- Audit role permissions regularly

```sql
-- Create application role
CREATE ROLE app_role;

-- Create read-only role
CREATE ROLE readonly;
GRANT CONNECT ON DATABASE app_db TO readonly;
GRANT USAGE ON SCHEMA public TO readonly;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO readonly;

-- Application user with appropriate role
CREATE USER app_user WITH PASSWORD 'secure_password';
GRANT app_role TO app_user;
```

### Database Maintenance

#### Regular Maintenance Tasks
1. VACUUM and ANALYZE (see Performance section)
2. Index maintenance
3. Database statistics updates
4. Table/index bloat monitoring

#### Automated Maintenance Script
```bash
#!/bin/bash
# Weekly database maintenance

LOGFILE="/var/log/postgres/maintenance_$(date +%Y%m%d).log"

echo "Maintenance started: $(date)" >> $LOGFILE

# Connect to database and perform maintenance
psql -d mydatabase -c "VACUUM VERBOSE ANALYZE;" >> $LOGFILE 2>&1
psql -d mydatabase -c "REINDEX DATABASE mydatabase;" >> $LOGFILE 2>&1

echo "Maintenance completed: $(date)" >> $LOGFILE
```

#### Managing Database Objects
- Regularly review large tables
- Monitor unused indexes
- Partition large tables when appropriate
- Set fillfactor appropriately (80-90% typically)

### High Availability and Replication

#### Replication Methods
- **Streaming Replication**: Built-in physical replication
- **Logical Replication**: Selective table-level replication
- **Third-party Solutions**: Patroni, Stolon, repmgr

#### Streaming Replication Setup
Primary server postgresql.conf:
```
wal_level = replica
max_wal_senders = 10
wal_keep_size = 1GB
```

Standby server recovery.conf (or postgresql.conf for newer versions):
```
primary_conninfo = 'host=primary_host port=5432 user=replication password=secret'
restore_command = 'cp /path/to/archive/%f %p'
```

#### Implementing Failover
- **Manual Failover**: Controlled promotion of standby
- **Automatic Failover**: Using tools like Patroni with etcd/consul
- Always test failover procedures regularly

#### Connection Routing Options
- Load balancers (HAProxy, Nginx)
- Connection poolers (PgBouncer)
- PostgreSQL-aware proxies (pgpool-II)

## Performance Tuning and Optimization

### System Resource Management

#### Hardware Considerations
- Use fast storage (SSD/NVMe) for data directories
- Provide sufficient RAM (minimum 8GB for production)
- Multiple CPU cores for better parallelism
- RAID 10 for mechanical disks; avoid RAID 5/6

#### Operating System Tuning
- Disable Transparent Huge Pages on Linux
- Adjust swappiness (vm.swappiness = 10)
- Increase shmmax and shmall values
- Configure I/O scheduler (deadline or noop for SSDs)

Linux sysctl settings for PostgreSQL:
```
# File: /etc/sysctl.conf
kernel.shmmax = 17179869184  # Half of RAM for large servers
kernel.shmall = 4194304      # shmmax/4096
vm.swappiness = 10
vm.dirty_background_ratio = 3
vm.dirty_ratio = 10
fs.aio-max-nr = 1048576
```

#### File System Considerations
- XFS or ext4 on Linux
- Align database cluster to file system block size
- Use noatime mount option
- Consider separate mount points for:
  - Database data
  - WAL files
  - Indexes
  - Temporary tablespace

### Memory Configuration

#### Key Memory Parameters
```
shared_buffers = 25% of RAM (up to 16GB)
work_mem = RAM / (max_connections * 2) 
maintenance_work_mem = 10% of RAM (up to 2GB)
effective_cache_size = 70% of RAM
```

#### Parameter Explanations
- **shared_buffers**: PostgreSQL's data cache
- **work_mem**: Memory for sort operations
- **maintenance_work_mem**: Memory for maintenance operations
- **effective_cache_size**: Estimate of OS cache available
- **temp_buffers**: Per-session temporary buffers

#### Memory Calculator Example
For a server with 32GB RAM and 100 expected connections:
```
shared_buffers = 8GB
work_mem = 32GB / (100 * 2) = 160MB
maintenance_work_mem = 1GB
effective_cache_size = 22GB
```

### Query Optimization

#### Analyzing Slow Queries
Enable query logging:
```
log_min_duration_statement = 1000  # Log queries taking > 1s
```

Use EXPLAIN ANALYZE:
```sql
EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM large_table WHERE id > 1000;
```

#### Query Optimization Techniques
- Use appropriate WHERE clauses
- Add proper indexes
- Avoid functions on indexed columns
- Rewrite complex queries
- Use LIMIT when appropriate

#### Common Query Anti-patterns
- SELECT * (fetch only needed columns)
- Full table scans on large tables
- Nested loops with large datasets
- Multiple successive queries (use JOINs)
- Functions on indexed columns in WHERE clauses

#### Example of Query Optimization
Before:
```sql
SELECT * FROM orders 
WHERE EXTRACT(YEAR FROM order_date) = 2023;
```

After:
```sql
SELECT * FROM orders 
WHERE order_date >= '2023-01-01' 
AND order_date < '2024-01-01';
```

### Indexing Strategies

#### Index Types
- B-tree (default, most common)
- Hash (equality operations)
- GiST (geometric, full-text search)
- GIN (full-text search, array operations)
- BRIN (block range, for large tables with natural ordering)

#### Creating Effective Indexes
```sql
-- Basic index
CREATE INDEX idx_customer_id ON orders(customer_id);

-- Compound index
CREATE INDEX idx_order_customer ON orders(order_date, customer_id);

-- Partial index
CREATE INDEX idx_recent_orders ON orders(order_date) 
WHERE order_date > CURRENT_DATE - INTERVAL '3 months';

-- Expression index
CREATE INDEX idx_lower_email ON customers(LOWER(email));
```

#### Index Maintenance
- Regularly REINDEX unused or bloated indexes
- Monitor index usage
- Remove redundant indexes

```sql
-- Check index usage
SELECT schemaname, relname, indexrelname, idx_scan, idx_tup_read
FROM pg_stat_user_indexes
JOIN pg_stat_user_tables ON pg_stat_user_indexes.relid = pg_stat_user_tables.relid
ORDER BY idx_scan DESC;

-- Finding unused indexes
SELECT schemaname, relname, indexrelname 
FROM pg_stat_user_indexes 
WHERE idx_scan = 0 AND indisunique IS FALSE;
```

### VACUUM and ANALYZE

#### Understanding VACUUM
- Reclaims storage from dead tuples
- Prevents transaction ID wraparound
- Updates visibility map
- Allows PostgreSQL to reuse space

#### VACUUM Types and Usage
```sql
-- Basic VACUUM (doesn't return space to OS)
VACUUM tablename;

-- Full VACUUM (returns space, locks table)
VACUUM FULL tablename;

-- VACUUM with analysis
VACUUM ANALYZE tablename;

-- ANALYZE only (update statistics)
ANALYZE tablename;
```

#### Autovacuum Configuration
```
autovacuum = on
autovacuum_max_workers = 3
autovacuum_naptime = 1min
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
autovacuum_vacuum_scale_factor = 0.2
autovacuum_analyze_scale_factor = 0.1
```

#### Table-Specific Autovacuum Settings
```sql
ALTER TABLE large_table SET (
  autovacuum_vacuum_scale_factor = 0.05,
  autovacuum_analyze_scale_factor = 0.025
);
```

### Connection Pooling

#### PgBouncer Setup
Example pgbouncer.ini:
```ini
[databases]
mydb = host=localhost port=5432 dbname=mydb

[pgbouncer]
listen_addr = *
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 100
```

#### Pooling Modes
- **Session**: Least efficient, most compatible
- **Transaction**: Good balance of efficiency and compatibility
- **Statement**: Most efficient, least compatible

#### Connection Management
- Monitor connection utilization
- Set max_connections appropriately
- Reserve superuser connections

### Monitoring Performance

#### Key Performance Metrics
- CPU usage
- Memory usage and swap
- Disk I/O performance
- Query execution times
- Transaction rates
- Cache hit ratios

#### PostgreSQL Statistics Views
```sql
-- Database statistics
SELECT * FROM pg_stat_database WHERE datname = 'mydb';

-- Table statistics
SELECT * FROM pg_stat_user_tables WHERE schemaname = 'public';

-- Index statistics
SELECT * FROM pg_stat_user_indexes WHERE schemaname = 'public';

-- Query statistics (pg_stat_statements extension)
SELECT query, calls, total_time, rows, shared_blks_hit, shared_blks_read
FROM pg_stat_statements
ORDER BY total_time DESC
LIMIT 10;
```

#### Setting Up pg_stat_statements
```
# Add to postgresql.conf
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.track = all
pg_stat_statements.max = 10000
```

```sql
-- After restarting PostgreSQL
CREATE EXTENSION pg_stat_statements;
```

#### Performance Dashboard Tools
- Grafana + Prometheus + postgres_exporter
- pgAdmin
- pg_activity
- pgHero

## Logging, Auditing, and Troubleshooting

### Logging Configuration

#### Essential Logging Parameters
```
log_destination = 'csvlog'
logging_collector = on
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_rotation_age = 1d
log_rotation_size = 100MB
log_min_messages = warning
log_min_error_statement = error
log_min_duration_statement = 1000  # Log queries taking > 1s
log_checkpoints = on
log_connections = on
log_disconnections = on
log_line_prefix = '%m [%p] %q%u@%d '
log_statement = 'none'  # Or 'ddl', 'mod', 'all'
```

#### Log Line Prefix Variables
- `%m`: Timestamp with milliseconds
- `%p`: Process ID
- `%q`: Session ID if silent, otherwise empty
- `%u`: User name
- `%d`: Database name
- `%h`: Remote host and port
- `%a`: Application name

#### Optimized Logging for Production
```
log_destination = 'csvlog'
logging_collector = on
log_directory = '/var/log/postgresql'
log_filename = 'postgresql-%Y-%m-%d.log'
log_rotation_age = 1d
log_truncate_on_rotation = on
log_min_duration_statement = 1000
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0
log_autovacuum_min_duration = 0
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
```

### Log Analysis

#### Key Log Events to Monitor
- Failed connection attempts
- Slow queries (over threshold)
- Deadlocks and lock timeouts
- Autovacuum operations
- Statement timeouts
- Checkpoint activity

#### Log Analysis Tools
- pgBadger: Comprehensive log analyzer
- GoAccess: Real-time log analyzer
- ELK Stack: Elasticsearch, Logstash, Kibana
- PostgreSQL Log Analyzer (pgLA)

#### pgBadger Example
```bash
# Generate daily report
pgbadger /var/log/postgresql/postgresql-2025-05-*.log -o /var/www/html/pgbadger/daily_report.html
```

#### DIY Log Analysis with SQL
```bash
# Convert PostgreSQL CSV log to table
cat postgresql-2025-05-14.csv | psql -c "COPY postgres_logs FROM STDIN WITH CSV;"
```

```sql
-- Finding slow queries
SELECT log_time, query_time, query
FROM postgres_logs
WHERE query_time > '1 second'::interval
ORDER BY query_time DESC;
```

### Auditing Database Activities

#### Built-in Audit Options
```
log_statement = 'all'  # Logs all SQL statements
log_min_duration_statement = 0  # Logs all queries with execution time
```

#### PostgreSQL Audit Extension (pgAudit)
Setup:
```
# In postgresql.conf
shared_preload_libraries = 'pgaudit'
pgaudit.log = 'all'
```

```sql
-- After restarting PostgreSQL
CREATE EXTENSION pgaudit;

-- Set up session logging
ALTER ROLE auditor SET pgaudit.log TO 'write, ddl';
```

#### Custom Audit Triggers
```sql
CREATE TABLE audit_log (
    id SERIAL PRIMARY KEY,
    table_name TEXT NOT NULL,
    user_name TEXT NOT NULL,
    action TEXT NOT NULL,
    original_data TEXT,
    new_data TEXT,
    query TEXT,
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE OR REPLACE FUNCTION audit_trigger_func()
RETURNS TRIGGER AS $$
BEGIN
    IF (TG_OP = 'DELETE') THEN
        INSERT INTO audit_log(table_name, user_name, action, original_data, query)
        VALUES (TG_TABLE_NAME, session_user, 'DELETE', row_to_json(OLD)::text, current_query());
        RETURN OLD;
    ELSIF (TG_OP = 'UPDATE') THEN
        INSERT INTO audit_log(table_name, user_name, action, original_data, new_data, query)
        VALUES (TG_TABLE_NAME, session_user, 'UPDATE', row_to_json(OLD)::text, row_to_json(NEW)::text, current_query());
        RETURN NEW;
    ELSIF (TG_OP = 'INSERT') THEN
        INSERT INTO audit_log(table_name, user_name, action, new_data, query)
        VALUES (TG_TABLE_NAME, session_user, 'INSERT', row_to_json(NEW)::text, current_query());
        RETURN NEW;
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

-- Apply to a table
CREATE TRIGGER audit_trigger_users
AFTER INSERT OR UPDATE OR DELETE ON users
FOR EACH ROW EXECUTE FUNCTION audit_trigger_func();
```

### Common Issues and Solutions

#### Connection Issues
- Check pg_hba.conf settings
- Verify network connectivity
- Ensure max_connections isn't reached
- Check for firewall rules

```sql
-- Current connection count
SELECT count(*) FROM pg_stat_activity;

-- Connection details
SELECT datname, usename, client_addr, backend_start, state
FROM pg_stat_activity;
```

#### Out of Disk Space
- Identify large tables and indexes
- Look for temporary files
- Check WAL directory space
- Implement table partitioning

```sql
-- Find largest tables
SELECT nspname || '.' || relname AS relation,
    pg_size_pretty(pg_total_relation_size(C.oid)) AS total_size
FROM pg_class C
LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
WHERE nspname NOT IN ('pg_catalog', 'information_schema')
    AND C.relkind <> 'i'
ORDER BY pg_total_relation_size(C.oid) DESC
LIMIT 20;
```

#### Lock Contention
- Check for idle transactions
- Look for long-running queries
- Investigate deadlocks

```sql
-- Find blocking locks
SELECT blocked_locks.pid AS blocked_pid,
       blocked_activity.usename AS blocked_user,
       blocking_locks.pid AS blocking_pid,
       blocking_activity.usename AS blocking_user,
       blocked_activity.query AS blocked_statement,
       blocking_activity.query AS blocking_statement
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype
  AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
  AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
  AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
  AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
  AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
  AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
  AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
  AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
  AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
  AND blocking_locks.pid != blocked_locks.pid
JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.granted;
```

#### High CPU Usage
- Look for non-optimized queries
- Check for missing indexes
- Investigate vacuum processes
- Consider connection pooling

#### Transaction ID Wraparound
- Monitor transaction age
- Schedule regular VACUUM
- Configure autovacuum appropriately

```sql
-- Check transaction age
SELECT datname, age(datfrozenxid) FROM pg_database ORDER BY age(datfrozenxid) DESC;

-- Find tables with old transaction IDs
SELECT c.oid::regclass, greatest(age(c.relfrozenxid), age(t.relfrozenxid)) as age
FROM pg_class c
LEFT JOIN pg_class t ON c.reltoastrelid = t.oid
WHERE c.relkind IN ('r', 'm')
ORDER BY age DESC
LIMIT 20;
```

### Diagnostic Tools

#### Built-in Tools
- pg_stat_activity
- pg_locks
- pg_stat_statements
- pg_stat_database
- pg_stat_user_tables

#### External Tools
- pg_top: Real-time monitoring tool
- pgcenter: Console-based PostgreSQL monitoring
- pgbadger: Log analyzer
- pg_stat_monitor: Enhanced monitoring extension

#### Using pg_top
```bash
pg_top -d mydatabase
```

#### Using pgcenter
```bash
pgcenter top -d mydatabase
```

### Performance Troubleshooting

#### Troubleshooting Process
1. Collect performance metrics
2. Identify bottlenecks (CPU, memory, disk, network)
3. Analyze slow queries
4. Review configuration parameters
5. Implement targeted optimizations
6. Measure improvements

#### Identifying Performance Issues
```sql
-- Top 10 longest running active queries
SELECT pid, now() - pg_stat_activity.query_start AS duration, query
FROM pg_stat_activity
WHERE state = 'active' AND now() - pg_stat_activity.query_start > interval '5 seconds'
ORDER BY duration DESC
LIMIT 10;

-- Tables with high read/write activity
SELECT schemaname, relname, 
       seq_scan, idx_scan, 
       n_tup_ins, n_tup_upd, n_tup_del,
       n_live_tup, n_dead_tup
FROM pg_stat_user_tables
ORDER BY (n_tup_ins + n_tup_upd + n_tup_del) DESC
LIMIT 10;

-- Tables with poor cache hit ratio
SELECT 
    relname, 
    heap_blks_read, 
    heap_blks_hit,
    CASE WHEN heap_blks_hit + heap_blks_read = 0 THEN 0
    ELSE heap_blks_hit::float / (heap_blks_hit + heap_blks_read) END AS hit_ratio
FROM pg_statio_user_tables
WHERE heap_blks_read > 0
ORDER BY hit_ratio ASC, heap_blks_read DESC
LIMIT 10;
```

#### Performance Issue Diagnosis Matrix

| Symptom | Possible Causes | Investigation | Solutions |
|---------|----------------|--------------|-----------|
| High CPU | Complex queries, missing indexes | pg_stat_statements, EXPLAIN ANALYZE | Add indexes, rewrite queries |
| High Memory | Undersized shared_buffers, large sorts | system monitoring, pg_stat_activity | Tune memory parameters |
| High Disk I/O | Table bloat, inefficient indexing | pg_statio_user_tables, system I/O stats | VACUUM, optimize indexes |
| Slow Queries | Poor indexing, statistics, joins | EXPLAIN ANALYZE, pg_stat_statements | Optimize queries, add indexes |
| Connection Problems | Too many connections, long-running transactions | pg_stat_activity | Connection pooling, transaction management |
| Database Bloat | Infrequent VACUUM | pg_stat_user_tables, pgstattuple | Configure autovacuum, VACUUM FULL |

#### Resolving Common Performance Problems

**Missing Indexes**
```sql
-- Find tables with frequent sequential scans
SELECT schemaname, relname, seq_scan, idx_scan,
    CASE WHEN idx_scan = 0 THEN 0 
    ELSE round((idx_scan::numeric/(idx_scan+seq_scan)::numeric)*100, 2) 
    END AS idx_scan_pct
FROM pg_stat_user_tables
WHERE (idx_scan + seq_scan) > 1000
ORDER BY idx_scan_pct ASC, seq_scan DESC
LIMIT 10;
```

**Buffer Cache Issues**
```sql
-- Check buffer cache hit ratio
SELECT 
    sum(heap_blks_read) as heap_read,
    sum(heap_blks_hit)  as heap_hit,
    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio
FROM pg_statio_user_tables;
```

**Slow Queries**
- Enable pg_stat_statements
- Use auto_explain extension
- Implement query performance monitoring
